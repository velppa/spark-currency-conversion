#+TITLE: Currency conversion using Spark

* Understand the problem

The challenge is to convert amount values for incoming incoming
transactions from transaction's currency to USD.

The conversion is a multiplication operation =amount_usd = amount *
rate_curr_usd=.

There are those constraints to care about:
1. Use a monthly updating FX rate based on the timestamp of each
   transaction.
2. The conversion should work for real-time (<50 ms) as well as batch
   (> 100M rows) transactions.
3. The real-time and batch conversion function should be identical,
   i.e. same inputs produce same outputs.

* Create a plan

Obtaining exchange rates from third parties should be independent
from using those rates in transaction processing. At this stage
we ensure the data quality, reshape rates into our internal model
and establish validity intervals along the time axis.

#+begin_src dot :file assets/flow.svg :exports results

digraph CurrencyConversion {
node[colorscheme=paired8]

subgraph cluster_Rates{
  style=invis
  RatesStorage [shape=cylinder]
  RatesLoader [shape=component; color=6]
}

{
  Transactions [shape=cylinder]
  TransactionsEnriched [shape=cylinder]
  TransactionsTopic [label="TransactionEnrichedStream"]

  TransacrionEnricherBatch [shape=component; color=6]
  TransacrionEnricherStream [shape=component; color=6]
}

RatesStorage -> RatesLoader [label="previous   "]
FXRatesProvider -> RatesLoader [label="increment   "]
RatesLoader -> RatesStorage [label="new   "]

Transactions -> TransactionEnricherBatch
RatesStorage -> TransactionEnricherBatch
TransactionEnricherBatch -> TransactionsEnriched

TransactionStream -> TransacrionEnricherStream
RatesStorage -> TransacrionEnricherStream
TransacrionEnricherStream -> TransactionsTopic
}

#+end_src

#+RESULTS:
[[file:assets/flow.svg]]

Red nodes are in scope of this assignment.

** Loading exchange rates

Exchange rates are loaded from an external provider at some infrequent
schedule (say every month), reshaped to our model and stored somewhere
durably. The easiest way is to use cloud object storage service like
Amazon S3, which gives durability, versioning and notifications about
changes.

The file would contain exchange rates from a currency to some base
currency like USD or EUR for a few hundreds of currencies.

Exchange rates should be updated at some interval, in the requirements
it's said to use monthly updating rate, but months are somewhat tricky
to use since they contain different number of days and February has
variadic length, and also the requirements might change to update
rates more often, so assume that rates would be updated at some fixed
interval in seconds like 4 weeks:

#+begin_src calc :exports both
interval_length = 4 * 7 * 24 * 60 * 60
#+end_src

#+RESULTS:
: interval_length = 2419200

When loading FX rates at moment T we set rates to be valid for the
next interval, thus ensuring that for a given transaction exchange
rate is always available. The following formula is used to get the
period of validity:

#+begin_src python :results verbatim :exports both
interval_length = 4 * 7 * 24 * 60 * 60
t = 1626436324
div, _ = divmod(t, interval_length)
return {
 'effective_from': (div+1)*interval_length,
 'effective_to': (div+2)*interval_length
}
#+end_src

#+RESULTS:
: {'effective_from': 1628121600, 'effective_to': 1630540800}

Let's use CSV format with the following schema:

| column         | datatype |            example | description                                    |
|----------------+----------+--------------------+------------------------------------------------|
| currency       | string   |                EUR | target currency                                |
| rate           | double   | 1.1797252419911401 | FX rate from currency to USD                   |
| created_at     | int      |         1626436324 | Unix timestamp when the record was inserted    |
| effective_from | int      |         1628121600 | Unix timestamp of the start period of validity |
| effective_to   | int      |         1630540800 | Unix timestamp of the end period of validity   |


We can estimate the number of records in the rates file:

#+begin_src python :results verbatim :exports both
currencies = 180
years_of_operation = 50
interval_length_days = 1/28.0
records = round(currencies * years_of_operation * 365 * interval_length_days)
bytes_per_record = 3 + 8*4 # one 3-char string and 4 numbers 8 bytes each

return {"records": records, "bytes": bytes_per_record*records}
#+end_src

#+RESULTS:
: {'records': 117321, 'bytes': 4106235}

So, the file with exchange rates would be tiny in comparison to
transactions, so we can load the whole file in memory (hash-map) and
experience O(1) access time.

For bulk processing, we'd load the file at startup time of the job,
for streaming processing we load the file at startup and also
subscribe to the topic with notifications that rates file is updated.
Upon receiving a notification, we load a new file into a temporary
buffer, then pause execution and atomically switch old hash-map to a
new buffer. I'm not sure how to do that in Spark Structured Streaming,
but in Go I'd use =map= with =sync.RWMutex{}= and lock the mutex for
writing when switching.


* Carry out the plan

** Install dependencies

- Clojure, refer to [[https://clojure.org/guides/getting_started#_installation_on_mac_via_homebrew][Installation on Mac via Homebrew]]:

  #+begin_src sh
brew install clojure/tools/clojure
  #+end_src

- Spark

  #+begin_src sh
brew install apache-spark
  #+end_src

- Scala

  #+begin_src sh
brew install scala@2.12
  #+end_src

  Then add =/opt/homebrew/opt/scala@2.12/bin= to =PATH=.

- Java

  #+begin_src sh
brew install java
  #+end_src

- sbt

  I manually downloaded from [[https://www.scala-sbt.org][sbt' website]] and unpacked to =/opt/sbt=
  since the version in brew doesn't support my MacBook M1.
  Add =/opt/sbt/bin= to =PATH=.

** Repository structure

- [[Makefile]] :: contains helper commands to run processing
- [[src/main/resources/application.conf]] :: contains configuration for applications
- [[src/main/clojure/rates.clj]] :: contains the code for RatesLoader
- [[src/main/scala/com/example/transactions/MainJob.scala]] :: contains
  code for Spark jobs
- [[transactions.csv]] :: input file with transactions, used by default as input to batch job
- [[rates.csv]] :: input file with history of FX rates, used by default as
  input to batch and stream jobs
- [[results]] :: the default directory where results will be written to
  (batch to =results/batch=, stream outputs to console at the moment)

** Rates Loader

It takes previous data and an increment (new rates), transforms
increment into target schema, filters out from previous data records
with =effective_from= equals to increment's =effective_from= and
appends increment to previous data and writes to rates storage.

On this stage in the production implementation we ensure data quality,
but we put it out of scope.

An example implementation is provided in Clojure under
[[src/main/clojure/rates.clj]].

To populate =rates.csv= file run:

#+begin_src sh
export CURRENCYLAYER_API_KEY="fbc......"
make load-rates
#+end_src

I back-filled the history of exchange rates for 40 periods of 28 days
back from today.

** Building and starting Spark Master

To build a jar, use:

#+begin_src sh
sbt clean compile assembly
#+end_src

To start a Spark Master use:

#+begin_src sh
spark-shell
#+end_src

** Enrichment logic

It takes transactions DataFrame and rates DataFrame and joins them by
=currency= and =effective_from= columns. The logic is shared between
bulk and stream processing.

The implementation is provided in =Enricher.withAmountUSD= method.

** Batch processing

It takes two files as input, constructs DataFrames out of them and runs
enrichment and saves results into a file.

To launch the processing:

#+begin_src sh
make run-batch
#+end_src

** Stream processing

It takes a file with rates as input and a directory where csv files
will be written, files are processed as a stream.

#+begin_src sh
make run-stream | grep -v InMemoryFileIndex
#+end_src

then in another shell generate files:

#+begin_src sh
head -n 250 transactions.csv > stream/1.csv
sleep 1
head -n 100 transactions.csv > stream/2.csv
sleep 2
head -n 1000 transactions.csv > stream/3.csv
sleep 3
head -n 50 transactions.csv > stream/4.csv
#+end_src

#+RESULTS:

* Look back

There are many things missing before the solution is production ready:

- Use a binary file format with embedded schema, like Parquet
- For streaming it's necessary to build reading of FX rates
- Change the logic from inner join to left join and divert rows where
  rate is null to a separate location for monitoring/future analysis
- Add tests
- Probably in Streaming mode instead of files we should read/write to
  Kafka topics
- Performance testing, it's not guaranteed that the solution will
  satisfy requirements 10ms/record for streaming
- Probably it would be better to rewrite to type-safe Datasets
- Parametrise interval for calculating validity windows for exchange
  rates
- Add DQ checks for incoming transactions
- Add monitoring/CI/CD
